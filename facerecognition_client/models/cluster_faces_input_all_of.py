# coding: utf-8

"""
    Dalet Media Mediator API

    # Scope Dalet Mediator API allows you to submit long running media jobs managed by Dalet services.  Long running media jobs include: - **Media processing** such as transcoding or automatic QC. - **Automatic metadata extraction** such as automatic speech transcription or face detection.  The Dalet Mediator API is a REST API with typed schema for the payload. # Architecture Job processing is performed on the cloud via dynamic combination of microservices. Dalet Mediator adopts the [EBU MCMA] architecture.  The key objectives of this architecture are to support: - Job management and monitoring - Long running transactions - Event based communication pattern - Service registration and discovery - Horizontal scalability in an elastic manner  The architecture is implemented using the serverless approach - relying on  independent microservices accessible through well documented REST endpoints and sharing a common object model. ## Roles The following services are involved in the processing of media jobs exposed through the Dalet Media Mediator API: - **Mediator**: this is the main entry point to the architecture; this API endpoint supports: 1. Checking authentication using an API key and a token mechanism 2. Verifying quota restrictions before accepting a submitted job 3. Keeping track of usage so that job processing can be tracked and billed 4. Keeping track of jobs metadata as a job repository - **Job Processor**: once a job request is accepted by the mediator, it is assigned to a Job Processor. The Job Processor dispatches the job to an appropriate Job Worker (depending on the job profile and other criteria such as load on the system and cost of operation).  It then keeps track of the progress of the job and its status until completion and possible failures and timeout.  It reports progress to the Mediator through notifications. - **Job Worker**: The Job Worker performs the actual work on the media object, for example, AI metadata extraction (AME) or essence transcoding.  It reports progress to the Job Processor through notifications. - **Service Registry**: The Service Registry keeps track of all active services in the architecture. It is queried by the Mediator and by Processors to discover candidate services to perform jobs.  It is updated whenever a new service is launched or stopped.  The Service Registry also stores the list of all job profiles supported by one of the Job Workers deployed in the architecture. The Dalet Mediator API abstracts away from the complexity of this orchestration and provides a simple endpoint to submit long running jobs and monitor the progress of their execution.  It serves as a facade for the additional technical services for authentication, usage monitoring and service registry.  [EBU MCMA]: /https://tech.ebu.ch/groups/mcma 'EBU MCMA' ## Job Lifecycle ![Job Lifecyle Diagram](./job_lifecycle.svg 'Job Lifecycle Diagram')  ## Authentication To use the Dalet Mediator API - you must obtain an APIKey from Dalet.  This key comes in the form of two parameters: * client ID * secret  Given these two parameters, a client program must first obtain an access token (GET /auth/access-token) and then associate this token to every subsequent calls.  When the token expires, the API will return a 401 error code.  In this case, the client must request a new token and resubmit the request.   # noqa: E501

    The version of the OpenAPI document: 1.4.0
    Contact: cortexsupport@dalet.com
    Generated by: https://openapi-generator.tech
"""


import pprint
import re  # noqa: F401

import six

from facerecognition_client.configuration import Configuration


class ClusterFacesInputAllOf(object):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    """
    Attributes:
      openapi_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    openapi_types = {
        'face_extraction_id': 'str',
        'cluster_collection_id': 'str',
        'minimum_cluster_size': 'float',
        'similarity_threshold': 'float'
    }

    attribute_map = {
        'face_extraction_id': 'faceExtractionId',
        'cluster_collection_id': 'clusterCollectionId',
        'minimum_cluster_size': 'minimumClusterSize',
        'similarity_threshold': 'similarityThreshold'
    }

    def __init__(self, face_extraction_id=None, cluster_collection_id='', minimum_cluster_size=2, similarity_threshold=0.5, local_vars_configuration=None):  # noqa: E501
        """ClusterFacesInputAllOf - a model defined in OpenAPI"""  # noqa: E501
        if local_vars_configuration is None:
            local_vars_configuration = Configuration()
        self.local_vars_configuration = local_vars_configuration

        self._face_extraction_id = None
        self._cluster_collection_id = None
        self._minimum_cluster_size = None
        self._similarity_threshold = None
        self.discriminator = None

        self.face_extraction_id = face_extraction_id
        if cluster_collection_id is not None:
            self.cluster_collection_id = cluster_collection_id
        if minimum_cluster_size is not None:
            self.minimum_cluster_size = minimum_cluster_size
        if similarity_threshold is not None:
            self.similarity_threshold = similarity_threshold

    @property
    def face_extraction_id(self):
        """Gets the face_extraction_id of this ClusterFacesInputAllOf.  # noqa: E501

        ID of a FaceCollection produced by an ExtractFaces job  # noqa: E501

        :return: The face_extraction_id of this ClusterFacesInputAllOf.  # noqa: E501
        :rtype: str
        """
        return self._face_extraction_id

    @face_extraction_id.setter
    def face_extraction_id(self, face_extraction_id):
        """Sets the face_extraction_id of this ClusterFacesInputAllOf.

        ID of a FaceCollection produced by an ExtractFaces job  # noqa: E501

        :param face_extraction_id: The face_extraction_id of this ClusterFacesInputAllOf.  # noqa: E501
        :type: str
        """
        if self.local_vars_configuration.client_side_validation and face_extraction_id is None:  # noqa: E501
            raise ValueError("Invalid value for `face_extraction_id`, must not be `None`")  # noqa: E501

        self._face_extraction_id = face_extraction_id

    @property
    def cluster_collection_id(self):
        """Gets the cluster_collection_id of this ClusterFacesInputAllOf.  # noqa: E501

        Optional field - if provided, cluster faces of the input collectionId into an existing clusterCollection, else create a new clusterCollection.  # noqa: E501

        :return: The cluster_collection_id of this ClusterFacesInputAllOf.  # noqa: E501
        :rtype: str
        """
        return self._cluster_collection_id

    @cluster_collection_id.setter
    def cluster_collection_id(self, cluster_collection_id):
        """Sets the cluster_collection_id of this ClusterFacesInputAllOf.

        Optional field - if provided, cluster faces of the input collectionId into an existing clusterCollection, else create a new clusterCollection.  # noqa: E501

        :param cluster_collection_id: The cluster_collection_id of this ClusterFacesInputAllOf.  # noqa: E501
        :type: str
        """

        self._cluster_collection_id = cluster_collection_id

    @property
    def minimum_cluster_size(self):
        """Gets the minimum_cluster_size of this ClusterFacesInputAllOf.  # noqa: E501

        Minimum number of images in a cluster.  Clusters with less candidates than this number will be filtered out.  # noqa: E501

        :return: The minimum_cluster_size of this ClusterFacesInputAllOf.  # noqa: E501
        :rtype: float
        """
        return self._minimum_cluster_size

    @minimum_cluster_size.setter
    def minimum_cluster_size(self, minimum_cluster_size):
        """Sets the minimum_cluster_size of this ClusterFacesInputAllOf.

        Minimum number of images in a cluster.  Clusters with less candidates than this number will be filtered out.  # noqa: E501

        :param minimum_cluster_size: The minimum_cluster_size of this ClusterFacesInputAllOf.  # noqa: E501
        :type: float
        """

        self._minimum_cluster_size = minimum_cluster_size

    @property
    def similarity_threshold(self):
        """Gets the similarity_threshold of this ClusterFacesInputAllOf.  # noqa: E501

        Similarity threshold (from 0 to 1) over which face candidates are grouped into a cluster.  Higher value produces more smaller clusters with higher confidence.  # noqa: E501

        :return: The similarity_threshold of this ClusterFacesInputAllOf.  # noqa: E501
        :rtype: float
        """
        return self._similarity_threshold

    @similarity_threshold.setter
    def similarity_threshold(self, similarity_threshold):
        """Sets the similarity_threshold of this ClusterFacesInputAllOf.

        Similarity threshold (from 0 to 1) over which face candidates are grouped into a cluster.  Higher value produces more smaller clusters with higher confidence.  # noqa: E501

        :param similarity_threshold: The similarity_threshold of this ClusterFacesInputAllOf.  # noqa: E501
        :type: float
        """

        self._similarity_threshold = similarity_threshold

    def to_dict(self):
        """Returns the model properties as a dict"""
        result = {}

        for attr, _ in six.iteritems(self.openapi_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value

        return result

    def to_str(self):
        """Returns the string representation of the model"""
        return pprint.pformat(self.to_dict())

    def __repr__(self):
        """For `print` and `pprint`"""
        return self.to_str()

    def __eq__(self, other):
        """Returns true if both objects are equal"""
        if not isinstance(other, ClusterFacesInputAllOf):
            return False

        return self.to_dict() == other.to_dict()

    def __ne__(self, other):
        """Returns true if both objects are not equal"""
        if not isinstance(other, ClusterFacesInputAllOf):
            return True

        return self.to_dict() != other.to_dict()
